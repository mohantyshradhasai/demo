{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohantyshradhasai/demo/blob/main/multimodalchatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdTJD8YYH9tm",
        "outputId": "74f6d5ad-a8a5-4b17-b6ed-8bcee8d48276"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/295.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.4/295.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install openai --quiet\n",
        "!pip install langchain --quiet\n",
        "!pip install cohere --quiet\n",
        "!pip install tiktoken --quiet\n",
        "!pip install langchain_community --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSgAftDBI2zU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ['COHERE_API_KEY'] = userdata.get(\"COHERE_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30K9oCUZI216"
      },
      "outputs": [],
      "source": [
        "from langchain import Cohere, ConversationChain\n",
        "from langchain.chat_models import ChatCohere\n",
        "from langchain.memory import ConversationBufferMemory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tE2Az3sCI24W",
        "outputId": "f14ef56e-7d10-4e1e-9208-4be642ba754b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your message:hi\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-439229803.py:4: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  conversation = ConversationChain(llm=llm,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI message ==>  Hello! How can I help you today? \n",
            "\n",
            "Please feel free to ask me anything you like, and I will do my best to respond with accurate and useful information. If there's something specific you want to know, go ahead and ask, and I'll do my best to provide a detailed and nuanced response drawing on the wealth of knowledge I have access to. \n",
            "\n",
            "I'm here to have a conversation with you and to provide information on a wide range of topics. So, feel free to ask me anything, and I will do my best to make our conversation informative, engaging, and collaborative. \n",
            "Your message: quit\n"
          ]
        }
      ],
      "source": [
        "user_input=input(\"Your message:\")\n",
        "llm= Cohere(temperature=0, max_tokens=512)\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(llm=llm,\n",
        "                                 memory=memory,\n",
        "                                 verbose=False\n",
        "                                 )\n",
        "while user_input!=\"quit\":\n",
        "    print(\"AI message ==>\", conversation.predict(input=user_input))\n",
        "    user_input=input(\"Your message: \")\n",
        "while user_input ==\"quit\":\n",
        "    memory.clear()\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xjtf7AhbKDq5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b797b97b-fa3d-49f1-c53d-c32bc99ba681"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/310.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m307.2/310.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#import pypdf\n",
        "!pip install pypdf --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0ptwLsZI26m"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "1t9MQrwjI293",
        "outputId": "015746bc-a6a4-4b88-b53b-48dd9aa64b34"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-979a9ecc-858e-44bd-9a53-fd841230563a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-979a9ecc-858e-44bd-9a53-fd841230563a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving NCDGWR2023.pdf to NCDGWR2023.pdf\n"
          ]
        }
      ],
      "source": [
        "#write a code to load document\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xERc6OK9LRsH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bae0408-6c5d-422b-de2c-89c44a7435f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded PDFs: ['NCDGWR2023.pdf']\n"
          ]
        }
      ],
      "source": [
        "pdf_files =list(uploaded.keys())\n",
        "print(\"Uploaded PDFs:\", pdf_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grMdxYM9LCSd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2155c9cb-75fc-49de-b631-10deee728b2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NCDGWR2023.pdf 460\n",
            "National Compilation on \n",
            "DYNAMIC GROUND WATER RESOURCES OF INDIA, 2023 \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "  \n",
            "Central Ground Water Board \n",
            "Department of Water Resources, \n",
            "River Development & Ganga Rejuvenation \n",
            "Ministry of Jal Shakti \n",
            "Government of India \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Faridabad \n",
            "September, 20236. q{. orftdr{fiq\n",
            "vEsel€ruo\n",
            "T. 5. Anitha Shyam\n",
            "Member (South)\n",
            "rfliftt $Iul,it\n",
            "wrrfihdrrcrq\n",
            "qo dsrtrq, rfl ftors oil{ riql {rdul frqrq\n",
            "irfic{ftvctrS\n",
            "Government of lndia\n",
            "1{inistry of JalShrkti\n",
            "oepartmant of Water Re3ourcer,\n",
            "River oevelopment and G.nga Reruvon.tion\n",
            "Cenkal Ground Water Board\n",
            "Trov\n",
            "3IlflIila -3qF rFrisa\n",
            "Preface\n",
            "Groundwater is a crucial resource, meeting to the water needs of agriculture, households, and industries in\n",
            "our nation. Unfortunately, its development occurs haphazardly through individual enkepreneurs in an\n",
            "unscientiflc manner, leading to over-exploitation in certain regions. To guarantee the sustainability of this\n",
            "vital resource, prudent management is essential. Making sound groundwate\n",
            "Lines: 16289\n",
            "Words: 99692\n",
            "Characters: 671268\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "full_text = \"\"\n",
        "for pdf_file in pdf_files:\n",
        "    loader = PyPDFLoader(pdf_file)\n",
        "    pages = loader.load()\n",
        "    print(pdf_file, len(pages))\n",
        "    for page in pages:\n",
        "        full_text += page.page_content\n",
        "\n",
        "print(full_text[:1000])\n",
        "print(\"Lines:\", len(full_text.split(\"\\n\")))\n",
        "print(\"Words:\", len(full_text.split(\" \")))\n",
        "print(\"Characters:\", len(full_text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqTvl1LYL9nP"
      },
      "source": [
        "splitting into chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WNxvTcNL6kw"
      },
      "outputs": [],
      "source": [
        "#splitting the text\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "chunks = text_splitter.split_documents(pages)\n",
        "\n",
        "print(len(chunks))\n",
        "print(chunks[0].page_content)  # access text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETjM9rCUL6nY"
      },
      "outputs": [],
      "source": [
        "!pip install ChromaDB --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXqPRb_aL6q2"
      },
      "outputs": [],
      "source": [
        "#creating embeddings\n",
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "import os\n",
        "os.environ[\"COHERE_API_KEY\"] = \"bc9dj6eNiAgkZ614WHn81q3yGQeeBdmjh7jUEd6N\"\n",
        "embeddings = CohereEmbeddings(\n",
        "    cohere_api_key=os.environ[\"COHERE_API_KEY\"],\n",
        "    model=\"embed-english-v3.0\",\n",
        "    user_agent=\"langchain\"\n",
        ")\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "arr_db = Chroma.from_documents(chunks, embeddings, persist_directory=\"arr_db\")\n",
        "arr_db.persist()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zilI93zxQLp2"
      },
      "outputs": [],
      "source": [
        "retriever = arr_db.as_retriever()\n",
        "result=retriever.get_relevant_documents(query=\"What is an array?Explain in 5 lines.\")\n",
        "for i in range(len(result)):\n",
        "  print(result[i].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--jXL-98QiAW"
      },
      "outputs": [],
      "source": [
        "#conversation and retreival chain\n",
        "# Cohere LLM\n",
        "llm = ChatCohere(cohere_api_key=os.environ[\"COHERE_API_KEY\"], model=\"command-r\")\n",
        "\n",
        "# Conversational chain\n",
        "conversational_rag = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=arr_db.as_retriever(),\n",
        "    return_source_documents=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-7G0pIRWlnS"
      },
      "outputs": [],
      "source": [
        "#import prompttemplate\n",
        "from langchain.prompts import PromptTemplate\n",
        "prompt_template = \"\"\"Text: {context}\n",
        "    Question: {question}\n",
        "    you are a chatbot designed to assist the users.\n",
        "    Answer only the questions based on the text provided. If the text doesn't contain the answer,\n",
        "    reply that the answer is not available.\n",
        "    keep the answers precise to the question\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "chain_type_kwargs = { \"prompt\" : PROMPT }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i7I7OHvUEgW"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade langchain langchain-cohere cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDChk5QLQ8VW"
      },
      "outputs": [],
      "source": [
        "llm=Cohere(model=\"command\", temperature=0)\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm = llm,\n",
        "        retriever= arr_db.as_retriever(),\n",
        "        memory = memory,\n",
        "        combine_docs_chain_kwargs=chain_type_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63S5o2ZjW8Y5"
      },
      "outputs": [],
      "source": [
        "user_input=input(\"Your message:\")\n",
        "while user_input!=\"quit\":\n",
        "    response=conversation_chain({\"question\": user_input})\n",
        "    print(\"AI message ==>\", response[\"answer\"])\n",
        "    user_input=input(\"Your message: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUld9VUQ8Mqv"
      },
      "outputs": [],
      "source": [
        "!pip install langchain cohere chromadb pytesseract pillow\n",
        "!sudo apt-get install tesseract-ocr -y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjsXC3C37D2b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import CohereEmbeddings\n",
        "from langchain.vectorstores import Chroma\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WW6cNNfl7D6V"
      },
      "outputs": [],
      "source": [
        "#upload\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "imagepath=list(uploaded.keys())[0]\n",
        "image=Image.open(imagepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGN2Sarn7D-O"
      },
      "outputs": [],
      "source": [
        "#extract the image to string\n",
        "text = pytesseract.image_to_string(image)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tz3gypaI7EA-"
      },
      "outputs": [],
      "source": [
        "#splitting into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "chunks = text_splitter.split_text(text)\n",
        "print(len(chunks))\n",
        "print(chunks[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLvmP2PM7EDO"
      },
      "outputs": [],
      "source": [
        "#creating vector database\n",
        "embeddings = CohereEmbeddings(\n",
        "    cohere_api_key=os.environ[\"COHERE_API_KEY\"],\n",
        "    model=\"embed-english-v3.0\",\n",
        "    user_agent=\"langchain\"\n",
        ")\n",
        "img_db=Chroma.from_texts(chunks, embeddings, persist_directory=\"img_db\")\n",
        "img_db.persist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taLTcowj-e9S"
      },
      "outputs": [],
      "source": [
        "retriever = img_db.as_retriever()\n",
        "result=retriever.get_relevant_documents(query=\"What is the dress?\")\n",
        "for i in range(len(result)):\n",
        "  print(result[i].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIzCsbNv_EG9"
      },
      "outputs": [],
      "source": [
        "llm=Cohere(model=\"command\", temperature=0)\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm = llm,\n",
        "        retriever= img_db.as_retriever(),\n",
        "        memory = memory,\n",
        "        combine_docs_chain_kwargs=chain_type_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1eNGVZc_ELg"
      },
      "outputs": [],
      "source": [
        "user_input=input(\"Your message:\")\n",
        "while user_input!=\"quit\":\n",
        "    response=conversation_chain({\"question\": user_input})\n",
        "    print(\"AI message ==>\", response[\"answer\"])\n",
        "    user_input=input(\"Your message: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyoabII5_EPH"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers transformers accelerate safetensors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-90FlJQ-fAW"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "#loading pipeline\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16)\n",
        "#generate image\n",
        "pipe=pipe.to(\"cuda\")\n",
        "image = pipe(\"an apple\").images[0]\n",
        "image.save(\"apple.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6gxfHJ2oGC6"
      },
      "outputs": [],
      "source": [
        "display(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "071940ae"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update && sudo apt-get install espeak -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yd0YsBsgFbHG"
      },
      "outputs": [],
      "source": [
        "#load SWivid/F5-TTS\n",
        "!pip install F5-TTS\n",
        "#uninstall F5-TTS\n",
        "!pip uninstall F5-TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hatNQXb0XzxK"
      },
      "outputs": [],
      "source": [
        "!pip install cohere gtts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jloxkdnyFbTU"
      },
      "outputs": [],
      "source": [
        "import cohere\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio\n",
        "#setup cohere\n",
        "co=cohere.Client('bc9dj6eNiAgkZ614WHn81q3yGQeeBdmjh7jUEd6N')\n",
        "#generate text using cohere\n",
        "documents = [\n",
        "    {\"title\": \"OS\", \"text\": \"Deadlock is a situation in operating systems where processes wait indefinitely.\"},\n",
        "    {\"title\": \"DBMS\", \"text\": \"Normalization reduces redundancy in databases.\"}\n",
        "]\n",
        "query=\"explain about deadlock situation.\"\n",
        "response = co.rerank(\n",
        "    query=query,\n",
        "    documents=[d[\"text\"] for d in documents],\n",
        "    top_n=1\n",
        ")\n",
        "context = response.results[0].document\n",
        "completion = co.generate(\n",
        "    model=\"command-r-plus\",\n",
        "    prompt=f\"Answer the question using this context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        ")\n",
        "\n",
        "answer = completion.generations[0].text.strip()\n",
        "print(\"Answer:\", answer)\n",
        "#convert to speech\n",
        "tts = gTTS(answer)\n",
        "tts.save(\"answer.mp3\")\n",
        "#play the audio\n",
        "Audio(\"answer.mp3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFxmibp3FbXH"
      },
      "outputs": [],
      "source": [
        "!pip install SpeechRecognition pydub\n",
        "\n",
        "import speech_recognition as sr\n",
        "from pydub import AudioSegment\n",
        "from google.colab import files\n",
        "\n",
        "# Upload file\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]   # get the uploaded filename\n",
        "\n",
        "# Convert MP3 -> WAV (mono, 16kHz)\n",
        "sound = AudioSegment.from_file(filename, format=\"mp3\")\n",
        "sound = sound.set_channels(1)       # mono\n",
        "sound = sound.set_frame_rate(16000) # 16 kHz\n",
        "wav_filename = \"converted.wav\"\n",
        "sound.export(wav_filename, format=\"wav\")\n",
        "\n",
        "# Initialize recognizer\n",
        "recognizer = sr.Recognizer()\n",
        "\n",
        "# Process audio\n",
        "with sr.AudioFile(wav_filename) as source:\n",
        "    audio = recognizer.record(source)   # use record() not listen()\n",
        "\n",
        "# Convert to text\n",
        "try:\n",
        "    text = recognizer.recognize_google(audio, language=\"en-US\")\n",
        "    print(\"You said:\", text)\n",
        "except sr.UnknownValueError:\n",
        "    print(\"Google Speech Recognition could not understand audio\")\n",
        "except sr.RequestError as e:\n",
        "    print(f\"Could not request results; {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKSDaVNF-fDZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# =========================\n",
        "# 1) Install dependencies\n",
        "# =========================\n",
        "!pip install -U gradio langchain langchain-community langchain-cohere cohere chromadb pypdf pillow pytesseract gtts SpeechRecognition pydub\n",
        "!pip install -U diffusers transformers accelerate safetensors\n",
        "!sudo apt-get install -y tesseract-ocr\n",
        "\n",
        "# =========================\n",
        "# 2) Imports & setup\n",
        "# =========================\n",
        "import os\n",
        "import gradio as gr\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "from pydub import AudioSegment\n",
        "import speech_recognition as sr\n",
        "import torch\n",
        "import cohere\n",
        "from gtts import gTTS\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "from langchain_cohere import ChatCohere\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "# ========= API KEY =========\n",
        "COHERE_API_KEY = \"bc9dj6eNiAgkZ614WHn81q3yGQeeBdmjh7jUEd6N\"  # <-- put your key here\n",
        "os.environ[\"COHERE_API_KEY\"] = COHERE_API_KEY\n",
        "co = cohere.Client(COHERE_API_KEY)\n",
        "\n",
        "# ========= Stable Diffusion (text→image) =========\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DTYPE = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
        "sd_pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"CompVis/stable-diffusion-v1-4\", torch_dtype=DTYPE\n",
        ").to(DEVICE)\n",
        "\n",
        "# ========= Global KB (PDFs + Images OCR text) =========\n",
        "embeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\n",
        "kb = None                                   # global Chroma DB\n",
        "persist_dir = \"kb_db\"                        # folder for persistence (optional)\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\", return_messages=True\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 3) Helper functions\n",
        "# =========================\n",
        "def _ensure_db():\n",
        "    \"\"\"Create empty Chroma if not exists.\"\"\"\n",
        "    global kb\n",
        "    if kb is None:\n",
        "        kb = Chroma.from_texts([], embeddings, persist_directory=persist_dir)\n",
        "\n",
        "def _add_text_to_kb(text: str) -> str:\n",
        "    \"\"\"Chunk text and add to vector store.\"\"\"\n",
        "    if not text or not text.strip():\n",
        "        return \"⚠️ No text to add.\"\n",
        "    _ensure_db()\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)\n",
        "    chunks = splitter.split_text(text)\n",
        "    if not chunks:\n",
        "        return \"⚠️ No chunks produced.\"\n",
        "    kb.add_texts(chunks)\n",
        "    try:\n",
        "        kb.persist()\n",
        "    except Exception:\n",
        "        pass\n",
        "    return f\"✅ Added {len(chunks)} chunks to knowledge base.\"\n",
        "\n",
        "# ---- PDF(s) → text → KB\n",
        "def add_pdfs_to_kb(pdf_files):\n",
        "    if not pdf_files:\n",
        "        return \"⚠️ Please upload at least one PDF.\"\n",
        "    collected = []\n",
        "    for f in pdf_files:\n",
        "        loader = PyPDFLoader(f.name if hasattr(f, \"name\") else f)\n",
        "        docs = loader.load()\n",
        "        collected.extend([d.page_content for d in docs if d.page_content])\n",
        "    text = \"\\n\".join(collected)\n",
        "    return _add_text_to_kb(text)\n",
        "\n",
        "# ---- Image(s) OCR → text → KB\n",
        "def add_images_to_kb(image_files):\n",
        "    if not image_files:\n",
        "        return \"⚠️ Please upload at least one image.\"\n",
        "    extracted = []\n",
        "    for f in image_files:\n",
        "        path = f.name if hasattr(f, \"name\") else f\n",
        "        img = Image.open(path)\n",
        "        extracted.append(pytesseract.image_to_string(img))\n",
        "    text = \"\\n\".join(extracted)\n",
        "    return _add_text_to_kb(text)\n",
        "\n",
        "# ---- RAG Chat (uses KB if available)\n",
        "def rag_chat(message, history):\n",
        "    _ensure_db()\n",
        "    llm = ChatCohere(model=\"command-r\", temperature=0)\n",
        "    chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=kb.as_retriever(search_kwargs={\"k\": 4}),\n",
        "        memory=memory,\n",
        "    )\n",
        "    resp = chain({\"question\": message})\n",
        "    return resp[\"answer\"]\n",
        "\n",
        "# ---- Normal chatbot (no KB)\n",
        "def normal_chat(message, history):\n",
        "    out = co.generate(model=\"command-r-plus\", prompt=message, max_tokens=256)\n",
        "    return out.generations[0].text.strip()\n",
        "\n",
        "# ---- Text → Image\n",
        "def text_to_image(prompt):\n",
        "    image = sd_pipe(prompt).images[0]\n",
        "    return image\n",
        "\n",
        "# ---- Text → Speech (returns filepath)\n",
        "def text_to_speech(text):\n",
        "    if not text or not text.strip():\n",
        "        return None\n",
        "    out_path = \"tts_output.mp3\"\n",
        "    gTTS(text).save(out_path)\n",
        "    return out_path\n",
        "\n",
        "# ---- Speech → Text (mp3/wav)\n",
        "def speech_to_text(audio_path):\n",
        "    if not audio_path:\n",
        "        return \"⚠️ Upload an audio file.\"\n",
        "    # convert to mono 16k wav\n",
        "    sound = AudioSegment.from_file(audio_path)\n",
        "    sound = sound.set_channels(1).set_frame_rate(16000)\n",
        "    wav_path = \"converted.wav\"\n",
        "    sound.export(wav_path, format=\"wav\")\n",
        "    r = sr.Recognizer()\n",
        "    with sr.AudioFile(wav_path) as src:\n",
        "        audio = r.record(src)\n",
        "    try:\n",
        "        return r.recognize_google(audio, language=\"en-US\")\n",
        "    except sr.UnknownValueError:\n",
        "        return \"❌ Could not understand audio.\"\n",
        "    except sr.RequestError as e:\n",
        "        return f\"❌ STT request error: {e}\"\n",
        "\n",
        "# ---- Clear KB\n",
        "def clear_kb():\n",
        "    global kb\n",
        "    kb = None\n",
        "    try:\n",
        "        import shutil\n",
        "        shutil.rmtree(persist_dir, ignore_errors=True)\n",
        "    except Exception:\n",
        "        pass\n",
        "    memory.clear()\n",
        "    return \"🧹 Cleared knowledge base and chat memory.\"\n",
        "\n",
        "# =========================\n",
        "# 4) Gradio UI\n",
        "# =========================\n",
        "with gr.Blocks(title=\"Multimodal Assistant\") as demo:\n",
        "    gr.Markdown(\"# 🧠 Multimodal AI Assistant\\n**PDF + Image OCR + RAG Chat + Normal Chat + Text→Image + STT + TTS**\")\n",
        "\n",
        "    with gr.Row():\n",
        "        clear_btn = gr.Button(\"Clear KB & Memory\", variant=\"stop\")\n",
        "        kb_status = gr.Markdown(\"\")\n",
        "        clear_btn.click(fn=clear_kb, outputs=kb_status)\n",
        "\n",
        "    with gr.Tab(\"📄 Add PDFs to Knowledge Base\"):\n",
        "        pdf_upl = gr.Files(label=\"Upload one or more PDFs\", file_types=[\".pdf\"])\n",
        "        pdf_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "        add_pdf_btn = gr.Button(\"Add to KB\")\n",
        "        add_pdf_btn.click(add_pdfs_to_kb, inputs=[pdf_upl], outputs=[pdf_status])\n",
        "\n",
        "    with gr.Tab(\"🖼 Add Images (OCR) to Knowledge Base\"):\n",
        "        img_upl = gr.Files(label=\"Upload one or more images\", file_types=[\".png\", \".jpg\", \".jpeg\"])\n",
        "        img_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "        add_img_btn = gr.Button(\"Add to KB\")\n",
        "        add_img_btn.click(add_images_to_kb, inputs=[img_upl], outputs=[img_status])\n",
        "\n",
        "    with gr.Tab(\"💬 Chat over Your KB (RAG)\"):\n",
        "        gr.Markdown(\"Ask questions grounded in your uploaded PDFs & images.\")\n",
        "        rag_chat_ui = gr.ChatInterface(\n",
        "            fn=rag_chat,\n",
        "            textbox=gr.Textbox(placeholder=\"Ask about your documents...\"),\n",
        "            title=\"RAG Chat\",\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"🗣️ Normal Chatbot (No KB)\"):\n",
        "        norm_chat_ui = gr.ChatInterface(\n",
        "            fn=normal_chat,\n",
        "            textbox=gr.Textbox(placeholder=\"Chat freely...\"),\n",
        "            title=\"Normal Chatbot\",\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"🎨 Text → Image\"):\n",
        "        prompt = gr.Textbox(label=\"Prompt\", placeholder=\"e.g., An apple wearing sunglasses, studio lighting\")\n",
        "        gen_img = gr.Image(label=\"Generated Image\")\n",
        "        gen_btn = gr.Button(\"Generate\")\n",
        "        gen_btn.click(text_to_image, inputs=prompt, outputs=gen_img)\n",
        "\n",
        "    with gr.Tab(\"🎤 Speech → Text\"):\n",
        "        stt_audio = gr.Audio(source=\"upload\", type=\"filepath\", label=\"Upload MP3/WAV\")\n",
        "        stt_text = gr.Textbox(label=\"Transcription\")\n",
        "        stt_audio.change(speech_to_text, inputs=stt_audio, outputs=stt_text)\n",
        "\n",
        "    with gr.Tab(\"🔊 Text → Speech\"):\n",
        "        tts_in = gr.Textbox(label=\"Text\")\n",
        "        tts_out = gr.Audio(type=\"filepath\", label=\"Speech\")\n",
        "        tts_btn = gr.Button(\"Speak\")\n",
        "        tts_btn.click(text_to_speech, inputs=tts_in, outputs=tts_out)\n",
        "\n",
        "demo.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68SQF4WH6y-4"
      },
      "outputs": [],
      "source": [
        "!pip install gradio torch transformers diffusers sentence-transformers pypdf gTTS SpeechRecognition pydub --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PF0IpQVq6zBF"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from gtts import gTTS\n",
        "import speech_recognition as sr\n",
        "from pypdf import PdfReader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAiZHhC36zE2"
      },
      "outputs": [],
      "source": [
        "# Text-to-Image\n",
        "sd_pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16\n",
        ")\n",
        "sd_pipe = sd_pipe.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Image-to-Text\n",
        "img_captioner = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "\n",
        "# Chatbot\n",
        "chatbot_pipe = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "# Speech recognizer\n",
        "recognizer = sr.Recognizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zr8IchHa7QhX"
      },
      "outputs": [],
      "source": [
        "# --- Text to Image ---\n",
        "def text_to_image(prompt):\n",
        "    return sd_pipe(prompt).images[0]\n",
        "\n",
        "# --- Image to Text ---\n",
        "def image_to_text(image):\n",
        "    return img_captioner(image)[0][\"generated_text\"]\n",
        "\n",
        "# --- Chatbot ---\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Load DialoGPT model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "\n",
        "chatbot_pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Function to maintain conversation\n",
        "llm = Cohere(temperature=0, max_tokens=512)\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=False)\n",
        "def chat_fn(user_input, history):\n",
        "    if user_input.lower() == \"quit\":\n",
        "        memory.clear()\n",
        "        return history + [[user_input, \"Conversation ended. Memory cleared.\"]]\n",
        "\n",
        "    response = conversation.predict(input=user_input)\n",
        "    history = history + [[user_input, response]]\n",
        "    return history\n",
        "\n",
        "def pdf_chat(pdf_file, question):\n",
        "    reader = PdfReader(pdf_file.name)\n",
        "    text = \" \".join([page.extract_text() for page in reader.pages])\n",
        "    qa_pipe = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "    answer = qa_pipe(question=question, context=text)\n",
        "    return answer[\"answer\"]\n",
        "\n",
        "# --- Speech to Text ---\n",
        "def speech_to_text(audio):\n",
        "    with sr.AudioFile(audio) as source:\n",
        "        audio_data = recognizer.record(source)\n",
        "        return recognizer.recognize_google(audio_data)\n",
        "\n",
        "# --- Text to Speech ---\n",
        "def text_to_speech(text):\n",
        "    tts = gTTS(text)\n",
        "    tts.save(\"output.mp3\")\n",
        "    return \"output.mp3\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00-JALlu7Qk8"
      },
      "outputs": [],
      "source": [
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# 🧠 Multimodal AI Assistant\")\n",
        "    with gr.Tab(\"💬Chatbot\"):\n",
        "      chatbot = gr.Chatbot()\n",
        "      msg = gr.Textbox(placeholder=\"Type a message...\")\n",
        "      clear = gr.Button(\"Clear Chat\")\n",
        "      def user_message(message, history):\n",
        "        return \"\", chat_fn(message, history)\n",
        "      msg.submit(user_message, [msg, chatbot], [msg, chatbot])\n",
        "      clear.click(lambda: (memory.clear(), []), None, chatbot, queue=False)\n",
        "    with gr.Tab(\"📄 PDF Q&A\"):\n",
        "      pdf_input = gr.File(type=\"filepath\", file_types=[\".pdf\"])  # ← changed here\n",
        "      question = gr.Textbox(label=\"Ask a question\")\n",
        "      pdf_output = gr.Textbox()\n",
        "      gr.Button(\"Get Answer\").click(pdf_chat, [pdf_input, question], pdf_output)\n",
        "\n",
        "\n",
        "    with gr.Tab(\"🖼️ Text → Image\"):\n",
        "        text_input = gr.Textbox(label=\"Enter prompt\")\n",
        "        img_output = gr.Image()\n",
        "        gr.Button(\"Generate\").click(text_to_image, text_input, img_output)\n",
        "\n",
        "    with gr.Tab(\"🖼️ Image → Text\"):\n",
        "        img_input = gr.Image(type=\"filepath\")\n",
        "        caption_output = gr.Textbox()\n",
        "        gr.Button(\"Describe\").click(image_to_text, img_input, caption_output)\n",
        "\n",
        "    with gr.Tab(\"🎤 Speech → Text\"):\n",
        "        audio_input = gr.Audio(sources=[\"microphone\", \"upload\"], type=\"filepath\")\n",
        "        stt_output = gr.Textbox()\n",
        "        gr.Button(\"Transcribe\").click(speech_to_text, audio_input, stt_output)\n",
        "\n",
        "    with gr.Tab(\"🗣️ Text → Speech\"):\n",
        "        tts_input = gr.Textbox()\n",
        "        tts_output = gr.Audio()\n",
        "        gr.Button(\"Speak\").click(text_to_speech, tts_input, tts_output)\n",
        "\n",
        "# Launch with a public link\n",
        "demo.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SysVQKNgqj3m"
      },
      "outputs": [],
      "source": [
        "!pip install gradio torch transformers diffusers sentence-transformers pypdf gTTS SpeechRecognition pydub --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jtetKmuqj6v"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from gtts import gTTS\n",
        "import speech_recognition as sr\n",
        "from pypdf import PdfReader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1gXb58rqj9Q"
      },
      "outputs": [],
      "source": [
        "# Text-to-Image\n",
        "sd_pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16\n",
        ")\n",
        "sd_pipe = sd_pipe.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Image-to-Text\n",
        "img_captioner = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "\n",
        "# Chatbot (DialoGPT)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "chatbot_pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Speech recognizer\n",
        "recognizer = sr.Recognizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUc-iW-oqkBS"
      },
      "outputs": [],
      "source": [
        "# --- Text to Image ---\n",
        "def text_to_image(prompt):\n",
        "    return sd_pipe(prompt).images[0]\n",
        "\n",
        "# --- Image to Text ---\n",
        "def image_to_text(image):\n",
        "    return img_captioner(image)[0][\"generated_text\"]\n",
        "\n",
        "# --- Chatbot with messages dict format ---\n",
        "def chatbot_fn(message, history):\n",
        "    history = history or []\n",
        "\n",
        "    # Build prompt from previous conversation\n",
        "    prompt = \"\"\n",
        "    for msg in history:\n",
        "        if msg[\"role\"] == \"user\":\n",
        "            prompt += f\"User: {msg['content']}\\n\"\n",
        "        else:\n",
        "            prompt += f\"Bot: {msg['content']}\\n\"\n",
        "    prompt += f\"User: {message}\\nBot:\"\n",
        "\n",
        "    # Generate response\n",
        "    response = chatbot_pipe(\n",
        "        prompt,\n",
        "        max_length=len(prompt.split()) + 50,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True\n",
        "    )[0][\"generated_text\"]\n",
        "\n",
        "    bot_reply = response.split(\"Bot:\")[-1].strip()\n",
        "\n",
        "    # Update history in dict format\n",
        "    history.append({\"role\": \"user\", \"content\": message})\n",
        "    history.append({\"role\": \"assistant\", \"content\": bot_reply})\n",
        "\n",
        "    return history, history\n",
        "\n",
        "# --- PDF Q&A ---\n",
        "def pdf_chat(pdf_file, question):\n",
        "    reader = PdfReader(pdf_file)\n",
        "    text = \" \".join([page.extract_text() for page in reader.pages])\n",
        "    qa_pipe = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "    answer = qa_pipe(question=question, context=text)\n",
        "    return answer[\"answer\"]\n",
        "\n",
        "# --- Speech to Text ---\n",
        "def speech_to_text(audio):\n",
        "    with sr.AudioFile(audio) as source:\n",
        "        audio_data = recognizer.record(source)\n",
        "        return recognizer.recognize_google(audio_data)\n",
        "\n",
        "# --- Text to Speech ---\n",
        "def text_to_speech(text):\n",
        "    tts = gTTS(text)\n",
        "    tts.save(\"output.mp3\")\n",
        "    return \"output.mp3\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlQ2Hf4Pq1K4"
      },
      "outputs": [],
      "source": [
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# 🧠 Multimodal AI Assistant\")\n",
        "\n",
        "    # --- Chatbot ---\n",
        "    with gr.Tab(\"💬 Chatbot\"):\n",
        "        chatbot = gr.Chatbot(type=\"messages\")  # <-- messages format\n",
        "        msg = gr.Textbox()\n",
        "        clear = gr.Button(\"Clear\")\n",
        "        msg.submit(chatbot_fn, [msg, chatbot], [chatbot, chatbot])\n",
        "        clear.click(lambda: [], None, chatbot, queue=False)\n",
        "\n",
        "    # --- PDF Q&A ---\n",
        "    with gr.Tab(\"📄 PDF Q&A\"):\n",
        "        pdf_input = gr.File(type=\"filepath\", file_types=[\".pdf\"])\n",
        "        question = gr.Textbox(label=\"Ask a question\")\n",
        "        pdf_output = gr.Textbox()\n",
        "        gr.Button(\"Get Answer\").click(pdf_chat, [pdf_input, question], pdf_output)\n",
        "\n",
        "    # --- Text to Image ---\n",
        "    with gr.Tab(\"🖼️ Text → Image\"):\n",
        "        text_input = gr.Textbox(label=\"Enter prompt\")\n",
        "        img_output = gr.Image()\n",
        "        gr.Button(\"Generate\").click(text_to_image, text_input, img_output)\n",
        "\n",
        "    # --- Image to Text ---\n",
        "    with gr.Tab(\"🖼️ Image → Text\"):\n",
        "        img_input = gr.Image(type=\"filepath\")\n",
        "        caption_output = gr.Textbox()\n",
        "        gr.Button(\"Describe\").click(image_to_text, img_input, caption_output)\n",
        "\n",
        "    # --- Speech to Text ---\n",
        "    with gr.Tab(\"🎤 Speech → Text\"):\n",
        "        audio_input = gr.Audio(sources=[\"microphone\", \"upload\"], type=\"filepath\")\n",
        "        stt_output = gr.Textbox()\n",
        "        gr.Button(\"Transcribe\").click(speech_to_text, audio_input, stt_output)\n",
        "\n",
        "    # --- Text to Speech ---\n",
        "    with gr.Tab(\"🗣️ Text → Speech\"):\n",
        "        tts_input = gr.Textbox()\n",
        "        tts_output = gr.Audio()\n",
        "        gr.Button(\"Speak\").click(text_to_speech, tts_input, tts_output)\n",
        "\n",
        "# Launch with temporary public link in Colab\n",
        "demo.launch(share=True, debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deUvpefPq1OD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaYcZAn87Qn7"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit pyngrok torch transformers diffusers sentence-transformers pypdf gTTS SpeechRecognition pydub --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhTbeadck8eK"
      },
      "outputs": [],
      "source": [
        "pip install langchain-community\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btaxMG9pkFGf"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "import os\n",
        "import PyPDF2\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import speech_recognition as sr\n",
        "from gtts import gTTS\n",
        "import tempfile\n",
        "\n",
        "# ========== Pipelines ==========\n",
        "chatbot_pipe = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "image_to_text_pipe = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "text_to_image_pipe = pipeline(\"text-to-image\", model=\"stabilityai/stable-diffusion-2\", torch_dtype=torch.float16)\n",
        "\n",
        "# ========== Functions ==========\n",
        "def chatbot_response(history, message):\n",
        "    response = chatbot_pipe(message, max_new_tokens=100, do_sample=True, temperature=0.7)[0][\"generated_text\"]\n",
        "    history.append((message, response))\n",
        "    return history, \"\"\n",
        "\n",
        "def tts_response(text):\n",
        "    tts = gTTS(text)\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as tmp:\n",
        "        tts.save(tmp.name)\n",
        "        return tmp.name\n",
        "\n",
        "def stt_response(audio):\n",
        "    recognizer = sr.Recognizer()\n",
        "    with sr.AudioFile(audio) as source:\n",
        "        audio_data = recognizer.record(source)\n",
        "        try:\n",
        "            return recognizer.recognize_google(audio_data)\n",
        "        except sr.UnknownValueError:\n",
        "            return \"Could not understand audio.\"\n",
        "        except sr.RequestError:\n",
        "            return \"API unavailable.\"\n",
        "\n",
        "def pdf_qa(pdf, question):\n",
        "    reader = PyPDF2.PdfReader(pdf.name)\n",
        "    text = \" \".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
        "    qa_pipe = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "    answer = qa_pipe({\"context\": text, \"question\": question})\n",
        "    return answer[\"answer\"]\n",
        "\n",
        "def image_to_text(img):\n",
        "    return image_to_text_pipe(img)[0]['generated_text']\n",
        "\n",
        "def text_to_image(prompt):\n",
        "    image = text_to_image_pipe(prompt).images[0]\n",
        "    out_path = tempfile.NamedTemporaryFile(delete=False, suffix=\".png\").name\n",
        "    image.save(out_path)\n",
        "    return out_path\n",
        "\n",
        "# ========== UI ==========\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# 🚀 Multi-Feature AI App (Colab)\")\n",
        "\n",
        "    with gr.Tab(\"💬 Chatbot\"):\n",
        "        chatbot = gr.Chatbot()\n",
        "        msg = gr.Textbox(label=\"Message\")\n",
        "        clear = gr.Button(\"Clear\")\n",
        "        msg.submit(chatbot_response, [chatbot, msg], [chatbot, msg])\n",
        "        clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "    with gr.Tab(\"🗣️ Text-to-Speech\"):\n",
        "        tts_in = gr.Textbox(label=\"Enter text\")\n",
        "        tts_out = gr.Audio()\n",
        "        tts_in.submit(tts_response, tts_in, tts_out)\n",
        "\n",
        "    with gr.Tab(\"🎤 Speech-to-Text\"):\n",
        "        stt_in = gr.Audio(sources=[\"microphone\"], type=\"filepath\")\n",
        "        stt_out = gr.Textbox()\n",
        "        stt_in.change(stt_response, stt_in, stt_out)\n",
        "\n",
        "    with gr.Tab(\"📄 PDF Q&A\"):\n",
        "        pdf_file = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
        "        question = gr.Textbox(label=\"Ask a question from the PDF\")\n",
        "        pdf_out = gr.Textbox()\n",
        "        question.submit(pdf_qa, [pdf_file, question], pdf_out)\n",
        "\n",
        "    with gr.Tab(\"🖼️ Image-to-Text\"):\n",
        "        img_in = gr.Image(type=\"pil\")\n",
        "        img_out = gr.Textbox()\n",
        "        img_in.upload(image_to_text, img_in, img_out)\n",
        "\n",
        "    with gr.Tab(\"🎨 Text-to-Image\"):\n",
        "        txt2img_in = gr.Textbox(label=\"Enter prompt\")\n",
        "        txt2img_out = gr.Image()\n",
        "        txt2img_in.submit(text_to_image, txt2img_in, txt2img_out)\n",
        "\n",
        "demo.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zgxLh5MkFJq"
      },
      "outputs": [],
      "source": [
        "# --- Chatbot ---\n",
        "def chatbot_fn(user_input, history):\n",
        "    if user_input.lower() == \"quit\":\n",
        "        memory.clear()\n",
        "        history.append([user_input, \"Conversation ended. Memory cleared.\"])\n",
        "        return history, history\n",
        "\n",
        "    reply = conversation.predict(input=user_input)\n",
        "    history = history + [[user_input, reply]]\n",
        "    return history, history\n",
        "\n",
        "# --- Text to Image ---\n",
        "def text_to_image(prompt):\n",
        "    return sd_pipe(prompt).images[0]\n",
        "\n",
        "# --- Image to Text ---\n",
        "def image_to_text(image):\n",
        "    return img_captioner(image)[0][\"generated_text\"]\n",
        "\n",
        "# --- PDF Q&A ---\n",
        "def pdf_chat(pdf_file, question):\n",
        "    reader = PdfReader(pdf_file)\n",
        "    text = \" \".join([page.extract_text() for page in reader.pages])\n",
        "    qa_pipe = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "    answer = qa_pipe(question=question, context=text)\n",
        "    return answer[\"answer\"]\n",
        "\n",
        "# --- Speech to Text ---\n",
        "def speech_to_text(audio):\n",
        "    with sr.AudioFile(audio) as source:\n",
        "        audio_data = recognizer.record(source)\n",
        "        return recognizer.recognize_google(audio_data)\n",
        "\n",
        "# --- Text to Speech ---\n",
        "def text_to_speech(text):\n",
        "    tts = gTTS(text)\n",
        "    tts.save(\"output.mp3\")\n",
        "    return \"output.mp3\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6797Gg0ckFMx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPfiIhUUKtVKXgPkOqm7sa3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}